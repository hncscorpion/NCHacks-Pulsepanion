from openai import OpenAI
import pandas as pd

client = OpenAI(api_key="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")

def generate_llm_summary(json_data: str, start_date: str, end_date: str, date_column: str = "date") -> str:
    """
    Generate a caregiver-friendly summary from health data JSON between given dates.

    Args:
        json_data (str): JSON string representing the patient health data.
        start_date (str): Start date in 'YYYY-MM-DD' format to filter data.
        end_date (str): End date in 'YYYY-MM-DD' format to filter data.
        date_column (str): Name of the date column in the data (default "date").

    Returns:
        str: Summary text generated by the LLM.
    """

    # Load JSON data into DataFrame
    df = pd.read_json(json_data)

    # Ensure date column is datetime type
    df[date_column] = pd.to_datetime(df[date_column])

    # Filter dataframe for the requested date range (inclusive)
    mask = (df[date_column] >= pd.to_datetime(start_date)) & (df[date_column] <= pd.to_datetime(end_date))
    filtered_df = df.loc[mask]

    if filtered_df.empty:
        return f"No data found between {start_date} and {end_date}. Please check the date range or data."

    # Convert filtered data to string for prompt (limit to 20 rows to avoid token overload)
    data_sample = filtered_df.head(20).to_string(index=False)

    prompt = f"""
Summarize the following caregiver health data between {start_date} and {end_date} in clear, caregiver-friendly language (6thâ€“8th grade level).  
Focus on important trends, anomalies, and suggestions to support caregiving.

Data:
{data_sample}
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant generating medical summaries for caregivers."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=300
    )

    return response.choices[0].message.content.strip()


